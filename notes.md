# Notes
* Each server in a Raft cluster may be in one of three states: leader, follower, or candidate.
* Under normal conditions, there is exactly one leader and all other servers are followers.
* Followers are passive - they issue no requests on their own but simply respond to requests from leaders and candidates.
* The leader handles all client requests (if a client contact6s a follower, the follower redirects it to the leader).
* The candidate state is used to elect a new leader.
* Raft divides time into *terms* of arbitrary length. Terms are numbered with consecutive integers. Each term begins with an *election*, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. If the election results in a split vote, then the term will end with no leader and a new term will begin.
* Each server stores a *current term* number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one of the servers current term is smaller than the other's, then it updates it current term to the larger value.
* If a candidate or a leader discovers that its term is out of date, it immediately reverts to the follower state. If a server receives a request from a stale term number, it rejects the request.

## Leader Election
* A heartbeat mechanism is used to trigger leader election.
* When servers start up, they begin as followers.
* A server remains in the follower state as long as it receives valid RPCs from a leader or candidate.
* Leaders send periodic heartbeats (AppendEntry RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication for a period of time called the *election timeout*, then it begins an election to choose a new leader.
* To begin an election, a follower increments its current term and transitions to the candidate state. It then votes for its self and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate remains in this state until one of the following is true:
    * It wins the election.
    * Another server establishes itself as leader
    * A period of time goes by with no winner.
* A candidate wins an election if it receives votes from the majority of the servers in the full cluster for the same term. Each server will vote for at most one candidate in a given term on a first-come-first-serve basis. Once a candidate wins an election, it becomes leader. It then sens heartbeat messages to all of the other servers to establish its authority and prevent new elections.
* When waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If the leader's term (included in its RPC) is at least as large as the candidates current term. then the candidate recognizes the leader as legitimate and returns to the follower state. Otherwise, the candidate rejects the RPC and continues in the candidate state.
* If a candidate neither wins nor loses the election (many followers could become candidates at the same time, thereby splitting the vote), then the candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. To prevent this from repeating indefinitely, election timeouts are chosen randomly from a fixed interval (150-300ms).

## Log Replication
* Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines. The leader appends the command to its a log as a new entry and then issues AppendEntries RPCs in parallel to the other servers to replicate the entry. When the entry has been safely replicated, the leader applies the entry to its state machine and returns the result of the that execution to the client.
* If a follower crashes, runs slowly, or packets are lost, the leader sends AppendEntries RPCs indefinitely until all followers eventually store all log entries.
* The leader decides when it is safe to apply a log entry to the state machines, such an entry is called *committed*. A log entry is committed once the leader that created the entry has replicated it on a majority of hte servers. This also commits all preceding entries in the leader's log, including entries by previous leaders.
* The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs (including heartbeats) so that the others servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).
* Note that leader failures may leave the log inconsistent. A follower may be missing entries that are present on the leader, may have extra entries that are not present on the leader, or both.
* The leader handles log inconsistencies by forcing the followers' log to duplicate its own. To bring a follower's log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any any entries in the follower's log after that point, and send the follower all of the leader entries after that point. All of these actions happen in response to the consistency check performed by AppendEntries RPCs.
* The leader maintains a *nextIndex* for each follower, which is the index of the next log entry that the leader will send to that follower. When a leader first comes to power, it initializes the nextIndex values to the index just after the last one in its log. If a follower's log is inconsistent with its leader's log, the AppendEntries consistency check will fail on the next AppendEntries RPC. After a rejection, the leader decrements the nextIndex and retries the AppendEntries RPC. Eventually, nextIndex will reach a point where the leader and follower logs match. When this happens, the AppendEntries RPC will succeed, which removes any conflicting entries from the follower's log and appends entries from the leader's log. Once AppendEntries succeeds, the follower's log is consistent with the leader's log.

## Safety
* If a leader crashes before before committing an entry, future leaders will attempt to finish replicating the entry. However, a leader cannot immediately conclude that an entry from the previous term is committed once it is stored on a majority of the servers. To eliminate this issue, Raft never commits log entries from previous terms by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the log matching property.
* If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully. If a crashes after completing the RPC but before responding, then it will receive the same RPC again before it restarts.

## Log Compaction

* Snapshotting is used to compact the log. In snapshotting, the entire current system state is written to a *snapshot* on stable storage, then the entire log up to that point is discarded.
* Each server takes snapshots independently, covering just the committed entries in its log. 
* Raft also includes a small amount of metadata in each snapshot: the *last included index* is the index of the last entry in the log that the snapshot replaces (the last entry that the state machine had applied), and the *last included term* is the term of this entry. These are preserved to support the AppendEntries consistency check for the first log entry following the snapshot, since that entry needs a previous log entry and term. To enable cluster membership changes, the snapshot also includes the latest configuration in the log as of last included index.
* Once a server completes writing its snapshot, it may delete all log entries up through the last include index, as well as any prior snapshot.
* Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower. The leader uses a InstallSnapshot RPC to send snapshots to followers that are too far behind. 
* When a follower receives a snapshot, it must decide what to do with its existing log entries. Usually the snapshot will contain new information not already in the recipient's log. In this case, the follower will discard its entire log; it is all superseded by the snapshot and may possibly have uncommitted entries that conflict with the snapshot. If instead the follower receives a snapshot that describes a prefix of its log, then the log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.